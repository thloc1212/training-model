{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14153892,"sourceType":"datasetVersion","datasetId":9021209}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    Wav2Vec2Processor, Wav2Vec2Model,\n    AutoFeatureExtractor, HubertModel,\n    WhisperFeatureExtractor, WhisperModel\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    confusion_matrix, accuracy_score, precision_score, \n    recall_score, f1_score\n)\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport torchaudio\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================\n# DEVICE & CONSTANTS\n# ============================================================\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"DEVICE = {DEVICE}\")\n\nDATA_DIR = \"/kaggle/input/vietnamese-speech-emotion-recognition-dataset\"\nOUTPUT_DIR = \"models\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nEMOTION_CLASSES = ['happy', 'neutral', 'sad', 'angry']\nEMOTION_MAP = {label: idx for idx, label in enumerate(EMOTION_CLASSES)}\n\n# Config for training\nCONFIG = {\n    'min_duration': 0.5,\n    'max_duration': 15.0,\n    'sample_rate': 16000,\n    'batch_size': 16,\n    'epochs': 5,\n    'warmup_steps': 500,\n    'learning_rate': 1e-5,\n    'weight_decay': 0.01,\n    'patience': 5,\n    'random_seed': 42,\n}\n\ntorch.manual_seed(CONFIG['random_seed'])\nnp.random.seed(CONFIG['random_seed'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"STEP 1: LOAD DATA FROM FOLDER STRUCTURE\")\nprint(\"=\"*60)\n\ndef load_audio_dataset(data_dir):\n    datasets = {}\n    \n    for split in ['train/phase1', 'train/phase2', 'val', 'test']:\n        split_path = os.path.join(data_dir, split)\n        data = []\n        \n        if not os.path.exists(split_path):\n            print(f\"Warning: {split_path} not found!\")\n            continue\n        \n        for emotion in EMOTION_CLASSES:\n            emotion_dir = os.path.join(split_path, emotion)\n            \n            if not os.path.exists(emotion_dir):\n                continue\n            \n            for filename in os.listdir(emotion_dir):\n                if filename.endswith('.wav'):\n                    filepath = os.path.join(emotion_dir, filename)\n                    try:\n                        # Load audio & calculate duration\n                        waveform, sr = torchaudio.load(filepath)\n                        \n                        # Resample if needed\n                        if sr != CONFIG['sample_rate']:\n                            waveform = torchaudio.functional.resample(\n                                waveform, sr, CONFIG['sample_rate']\n                            )\n                            sr = CONFIG['sample_rate']\n                        \n                        duration = waveform.shape[1] / sr\n                        \n                        # Filter by duration\n                        if CONFIG['min_duration'] <= duration <= CONFIG['max_duration']:\n                            data.append({\n                                'audio_path': filepath,\n                                'emotion': emotion,\n                                'emotion_id': EMOTION_MAP[emotion],\n                                'duration': duration,\n                                'split': split.split('/') if '/' in split else split\n                            })\n                    except Exception as e:\n                        print(f\"Error loading {filepath}: {e}\")\n        \n        if data:\n            datasets[split] = pd.DataFrame(data)\n            print(f\"{split}: {len(data)} files\")\n            print(f\"  Emotion distribution: {datasets[split]['emotion'].value_counts().to_dict()}\")\n    \n    return datasets\n\ndatasets = load_audio_dataset(DATA_DIR)\n\n# Check if we have data\ndf_phase1 = datasets.get('train/phase1', pd.DataFrame())\ndf_phase2 = datasets.get('train/phase2', pd.DataFrame())\ndf_val = datasets.get('val', pd.DataFrame())\ndf_test = datasets.get('test', pd.DataFrame())\n\nprint(f\"\\nTotal samples:\")\nprint(f\"  Phase1: {len(df_phase1)}\")\nprint(f\"  Phase2: {len(df_phase2)}\")\nprint(f\"  Val: {len(df_val)}\")\nprint(f\"  Test: {len(df_test)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"STEP 2: DATASET CLASS\")\nprint(\"=\"*60)\n\nclass AudioEmotionDataset(Dataset):\n    \"\"\"Load audio files and compute features\"\"\"\n    def __init__(self, df, processor=None, feature_extractor=None, mode='wav2vec'):\n        self.df = df.reset_index(drop=True)\n        self.processor = processor\n        self.feature_extractor = feature_extractor\n        self.mode = mode\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        audio_path = row['audio_path']\n        label = row['emotion_id']\n        \n        # Load audio\n        waveform, sr = torchaudio.load(audio_path)\n        \n        # Ensure mono\n        if waveform.shape[0] > 1:\n            waveform = waveform.mean(0, keepdim=True)\n        \n        waveform = waveform.squeeze(0).numpy().astype(np.float32)\n        \n        # Resample if needed\n        if sr != CONFIG['sample_rate']:\n            waveform = torchaudio.functional.resample(\n                torch.from_numpy(waveform), sr, CONFIG['sample_rate']\n            ).numpy()\n        \n        return {\n            'waveform': waveform,\n            'labels': label\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nSetting up collate functions...\")\n\n# WAV2VEC2 COLLATE\nwav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n\ndef collate_wav2vec(batch):\n    waveforms = [item['waveform'] for item in batch]\n    labels = [item['labels'] for item in batch]\n    \n    # Process with padding\n    padded = wav2vec_processor(\n        waveforms,\n        sampling_rate=CONFIG['sample_rate'],\n        return_tensors=\"pt\",\n        padding=True\n    )\n    \n    return {\n        'input_values': padded['input_values'],\n        'labels': torch.tensor(labels, dtype=torch.long)\n    }\n\n# HUBERT COLLATE\nhubert_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n\ndef collate_hubert(batch):\n    waveforms = [item['waveform'] for item in batch]\n    labels = [item['labels'] for item in batch]\n    \n    # Extract features with padding\n    features = hubert_extractor(\n        waveforms,\n        sampling_rate=CONFIG['sample_rate'],\n        return_tensors=\"pt\",\n        padding=True\n    )\n    \n    return {\n        'input_values': features['input_values'],\n        'labels': torch.tensor(labels, dtype=torch.long)\n    }\n\n# WHISPER COLLATE\nwhisper_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n\ndef collate_whisper(batch):\n    waveforms = [item['waveform'] for item in batch]\n    labels = [item['labels'] for item in batch]\n    \n    # Extract mel spectrogram\n    features = whisper_extractor(\n        waveforms,\n        sampling_rate=CONFIG['sample_rate'],\n        return_tensors=\"pt\"\n    )\n    \n    # Pad to max length\n    input_features = features['input_features']\n    max_len = 3000\n    \n    padded = []\n    for feat in input_features:\n        if feat.shape[-1] < max_len:\n            pad = torch.nn.functional.pad(\n                feat, (0, max_len - feat.shape[-1]), mode='constant', value=0\n            )\n        else:\n            pad = feat[:, :max_len]\n        padded.append(pad)\n    \n    return {\n        'input_features': torch.stack(padded),\n        'labels': torch.tensor(labels, dtype=torch.long)\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nLoading model architectures...\")\n\n# --- MODEL 1: Wav2Vec2 + MLP ---\nclass Wav2VecClassifier(nn.Module):\n    def __init__(self, num_labels=4):\n        super().__init__()\n        self.encoder = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n        hidden_size = self.encoder.config.hidden_size  # 768\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(128, num_labels)\n        )\n    \n    def forward(self, input_values, attention_mask=None, labels=None):\n        # Encode\n        out = self.encoder(input_values=input_values, attention_mask=attention_mask)\n        hidden = out.last_hidden_state  # [B, T, 768]\n        \n        # Mean pooling\n        if attention_mask is not None:\n            # Mask padding tokens\n            mask = attention_mask.unsqueeze(-1).float()  # [B, T, 1]\n            pooled = (hidden * mask).sum(1) / mask.sum(1)\n        else:\n            pooled = hidden.mean(dim=1)\n        \n        # Classify\n        logits = self.classifier(pooled)\n        \n        loss = None\n        if labels is not None:\n            loss = nn.CrossEntropyLoss()(logits, labels)\n        \n        return {'logits': logits, 'loss': loss}\n\n# --- MODEL 2: HuBERT + CNN ---\nclass HubertCNNClassifier(nn.Module):\n    def __init__(self, num_labels=4):\n        super().__init__()\n        self.encoder = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n        hidden_size = self.encoder.config.hidden_size  # 768\n        \n        # CNN 1D head\n        self.conv1 = nn.Conv1d(hidden_size, 256, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.conv2 = nn.Conv1d(256, 128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm1d(128)\n        \n        self.fc = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, num_labels)\n        )\n    \n    def forward(self, input_values, attention_mask=None, labels=None):\n        # Encode\n        out = self.encoder(input_values=input_values, attention_mask=attention_mask)\n        hidden = out.last_hidden_state  # [B, T, 768]\n        \n        # CNN: transpose [B, T, 768] -> [B, 768, T]\n        x = hidden.transpose(1, 2)\n        \n        # Conv blocks\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        \n        # Global max pooling\n        x = torch.max(x, dim=2).values  # [B, 128]\n        \n        # Classify\n        logits = self.fc(x)\n        \n        loss = None\n        if labels is not None:\n            loss = nn.CrossEntropyLoss()(logits, labels)\n        \n        return {'logits': logits, 'loss': loss}\n\n# --- MODEL 3: Whisper + Attention ---\nclass WhisperAttentionClassifier(nn.Module):\n    def __init__(self, num_labels=4):\n        super().__init__()\n        self.encoder = WhisperModel.from_pretrained(\"openai/whisper-tiny\").encoder\n        hidden_size = 384  # Whisper-tiny\n        \n        # Attention layer\n        self.attn_query = nn.Linear(hidden_size, 1, bias=False)\n        \n        # Classification head\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_size, 128),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(128, num_labels)\n        )\n    \n    def forward(self, input_features, labels=None):\n        # Encode\n        out = self.encoder(input_features=input_features)\n        hidden = out.last_hidden_state  # [B, T, 384]\n        \n        # Attention\n        attn_scores = self.attn_query(hidden)  # [B, T, 1]\n        attn_weights = F.softmax(attn_scores, dim=1)  # [B, T, 1]\n        \n        # Context vector\n        context = (attn_weights * hidden).sum(dim=1)  # [B, 384]\n        \n        # Classify\n        logits = self.fc(context)\n        \n        loss = None\n        if labels is not None:\n            loss = nn.CrossEntropyLoss()(logits, labels)\n        \n        return {'logits': logits, 'loss': loss}\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_matrix(self, labels, preds, normalize=True):\n    cm = confusion_matrix(labels, preds)\n\n    if normalize:\n        cm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt=\".2f\" if normalize else \"d\",\n        cmap=\"Blues\",\n        xticklabels=EMOTION_CLASSES,\n        yticklabels=EMOTION_CLASSES\n    )\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(f\"{self.model_name} - Confusion Matrix\")\n\n    plt.tight_layout()\n    plt.savefig(f\"{OUTPUT_DIR}/{self.model_name}_confusion_matrix.png\")\n    plt.close()\n\n\ndef plot_learning_curves(self):\n    epochs = range(1, len(self.history['train_loss']) + 1)\n\n    plt.figure(figsize=(14, 4))\n\n    # Loss\n    plt.subplot(1, 3, 1)\n    plt.plot(epochs, self.history['train_loss'], marker='o', label='Train')\n    plt.plot(epochs, self.history['val_loss'], marker='o', label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss')\n    plt.legend()\n\n    # Accuracy\n    plt.subplot(1, 3, 2)\n    plt.plot(epochs, self.history['train_acc'], marker='o', label='Train')\n    plt.plot(epochs, self.history['val_acc'], marker='o', label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Accuracy')\n    plt.legend()\n\n    # F1\n    plt.subplot(1, 3, 3)\n    plt.plot(epochs, self.history['train_f1'], marker='o', label='Train')\n    plt.plot(epochs, self.history['val_f1'], marker='o', label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel('F1-score')\n    plt.title('F1-score')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.savefig(f\"{OUTPUT_DIR}/{self.model_name}_learning_curve.png\")\n    plt.close()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nDefining training function...\")\n\nimport matplotlib.pyplot as plt\n\nclass Trainer:\n    def __init__(self, model, train_loader, val_loader, optimizer, device, model_name, patience=5):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.optimizer = optimizer\n        self.device = device\n        self.model_name = model_name\n        self.patience = patience\n        self.history = {\n            'train_loss': [], 'train_acc': [], 'train_f1': [],\n            'val_loss': [], 'val_acc': [], 'val_f1': []\n        }\n        self.best_val_acc = 0\n        self.patience_counter = 0\n        self.last_val_preds = None\n        self.last_val_labels = None\n\n    \n    def train_epoch(self):\n        self.model.train()\n        train_loss = 0\n        all_preds = []\n        all_labels = []\n        \n        for batch in tqdm(self.train_loader, desc=\"Training\", leave=False):\n            batch = {k: v.to(self.device) for k, v in batch.items() if k != 'attention_mask'}\n            \n            self.optimizer.zero_grad()\n            outputs = self.model(**batch)\n            loss = outputs['loss']\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.optimizer.step()\n            \n            train_loss += loss.item()\n            preds = torch.argmax(outputs['logits'], dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['labels'].cpu().numpy())\n        \n        train_acc = accuracy_score(all_labels, all_preds)\n        train_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n        \n        return train_loss / len(self.train_loader), train_acc, train_f1\n    \n    def eval_epoch(self):\n        self.model.eval()\n        val_loss = 0\n        all_preds = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for batch in tqdm(self.val_loader, desc=\"Evaluating\", leave=False):\n                batch = {k: v.to(self.device) for k, v in batch.items() if k != 'attention_mask'}\n                \n                outputs = self.model(**batch)\n                loss = outputs['loss']\n                \n                val_loss += loss.item()\n                preds = torch.argmax(outputs['logits'], dim=1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(batch['labels'].cpu().numpy())\n        \n        val_acc = accuracy_score(all_labels, all_preds)\n        val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n        \n        return val_loss / len(self.val_loader), val_acc, val_f1, all_preds, all_labels\n    \n    def plot_learning_curves(self):\n        epochs = range(1, len(self.history['train_loss']) + 1)\n        \n        plt.figure(figsize=(12, 4))\n        \n        # --------- Loss ----------\n        plt.subplot(1, 3, 1)\n        plt.plot(epochs, self.history['train_loss'], label='Train Loss')\n        plt.plot(epochs, self.history['val_loss'], label='Val Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title(f'{self.model_name} - Loss')\n        plt.legend()\n        \n        # --------- Accuracy ----------\n        plt.subplot(1, 3, 2)\n        plt.plot(epochs, self.history['train_acc'], label='Train Acc')\n        plt.plot(epochs, self.history['val_acc'], label='Val Acc')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.title(f'{self.model_name} - Accuracy')\n        plt.legend()\n        \n        # --------- F1 ----------\n        plt.subplot(1, 3, 3)\n        plt.plot(epochs, self.history['train_f1'], label='Train F1')\n        plt.plot(epochs, self.history['val_f1'], label='Val F1')\n        plt.xlabel('Epoch')\n        plt.ylabel('F1-score')\n        plt.title(f'{self.model_name} - F1')\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.savefig(f\"{OUTPUT_DIR}/{self.model_name}_learning_curve.png\")\n        plt.close()\n    \n    def train(self, epochs):\n        for epoch in range(epochs):\n            train_loss, train_acc, train_f1 = self.train_epoch()\n            val_loss, val_acc, val_f1, _, _ = self.eval_epoch()\n            \n            self.history['train_loss'].append(train_loss)\n            self.history['train_acc'].append(train_acc)\n            self.history['train_f1'].append(train_f1)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_acc'].append(val_acc)\n            self.history['val_f1'].append(val_f1)\n            \n            print(f\"Epoch {epoch+1}/{epochs} | \"\n                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} F1: {train_f1:.4f} | \"\n                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} F1: {val_f1:.4f}\")\n            \n            # Early stopping\n            if val_acc > self.best_val_acc:\n                self.best_val_acc = val_acc\n                self.patience_counter = 0\n                torch.save(self.model.state_dict(), f\"{OUTPUT_DIR}/{self.model_name}_best.pt\")\n            else:\n                self.patience_counter += 1\n                if self.patience_counter >= self.patience:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n        \n        # vẽ learning curve sau khi train xong\n        self.plot_learning_curves()\n        \n        return self.history\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Freezing backbone và unfreeze 3 layers cuối ---\n\ndef freeze_backbone_except_last_n(model, n=3, model_type='wav2vec'):\n    \"\"\"\n    model: nn.Module (Wav2Vec2Classifier / HubertCNNClassifier / WhisperAttentionClassifier)\n    n: số layer cuối để unfreeze\n    model_type: 'wav2vec', 'hubert', 'whisper'\n    \"\"\"\n    #todo demo\n    if model_type == 'wav2vec':\n        for param in model.encoder.parameters():\n            param.requires_grad = False\n        for layer in model.encoder.encoder.layers[-n:]:\n            for param in layer.parameters():\n                param.requires_grad = True\n        for param in model.classifier.parameters():\n            param.requires_grad = True\n    \n    elif model_type == 'hubert':\n        for param in model.encoder.parameters():\n            param.requires_grad = False\n        for layer in model.encoder.encoder.layers[-n:]:\n            for param in layer.parameters():\n                param.requires_grad = True\n        for param in model.fc.parameters():\n            param.requires_grad = True\n    \n    elif model_type == 'whisper':\n        for param in model.encoder.parameters():\n            param.requires_grad = False\n        # mở n layer cuối encoder\n        for layer in model.encoder.layers[-n:]:\n            for param in layer.parameters():\n                param.requires_grad = True\n        for param in model.fc.parameters():\n            param.requires_grad = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"STEP 6: SINGLE RANDOM SHUFFLE SPLIT ON PHASE1 (NO STRATIFY)\")\nprint(\"=\"*60)\n\n# ============================================================\n# SINGLE RANDOM SPLIT (ONE TIME ONLY)\n# ============================================================\ntest_size = 0.2  # 20% for evaluation\nn_samples = len(df_phase1)\nn_eval = int(n_samples * test_size)\n\nrng = np.random.RandomState(CONFIG['random_seed'])\nindices = rng.permutation(n_samples)\n\neval_idx = indices[:n_eval]\ntrain_idx = indices[n_eval:]\n\ndf_train_split = df_phase1.iloc[train_idx].reset_index(drop=True)\ndf_eval_split = df_phase1.iloc[eval_idx].reset_index(drop=True)\n\nprint(f\"Train: {len(df_train_split)} samples\")\nprint(f\"Eval: {len(df_eval_split)} samples\")\n\n# ============================================================\n# INIT RESULT CONTAINER (NO LISTS)\n# ============================================================\nresults = {\n    'wav2vec': {},\n    'hubert': {},\n    'whisper': {}\n}\n\n# ============================================================\n# MODEL 1: WAV2VEC 2.0\n# ============================================================\nprint(f\"\\n{'*'*40}\")\nprint(\"MODEL 1: WAV2VEC 2.0\")\nprint(f\"{'*'*40}\")\n\ndataset_train = AudioEmotionDataset(\n    df_train_split, processor=wav2vec_processor, mode='wav2vec'\n)\ndataset_eval = AudioEmotionDataset(\n    df_eval_split, processor=wav2vec_processor, mode='wav2vec'\n)\n\nloader_train = DataLoader(\n    dataset_train,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    collate_fn=collate_wav2vec\n)\nloader_eval = DataLoader(\n    dataset_eval,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    collate_fn=collate_wav2vec\n)\n\nmodel1 = Wav2VecClassifier(num_labels=4).to(DEVICE)\nfreeze_backbone_except_last_n(model1, n=1, model_type='wav2vec')\n\noptimizer1 = torch.optim.AdamW(\n    model1.parameters(),\n    lr=CONFIG['learning_rate'],\n    weight_decay=CONFIG['weight_decay']\n)\n\ntrainer1 = Trainer(\n    model1,\n    loader_train,\n    loader_eval,\n    optimizer1,\n    DEVICE,\n    \"wav2vec_single_split\",\n    patience=CONFIG['patience']\n)\ntrainer1.train(CONFIG['epochs'])\n\nmodel1.eval()\nall_preds, all_labels = [], []\n\nwith torch.no_grad():\n    for batch in loader_eval:\n        batch = {k: v.to(DEVICE) for k, v in batch.items() if k != 'attention_mask'}\n        outputs = model1(**batch)\n        preds = torch.argmax(outputs['logits'], dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(batch['labels'].cpu().numpy())\n\nresults['wav2vec']['accuracy'] = accuracy_score(all_labels, all_preds)\nresults['wav2vec']['precision'] = precision_score(all_labels, all_preds, average='macro', zero_division=0)\nresults['wav2vec']['recall'] = recall_score(all_labels, all_preds, average='macro', zero_division=0)\nresults['wav2vec']['f1'] = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n\nprint(f\"WAV2VEC Acc: {results['wav2vec']['accuracy']:.4f} | \"\n      f\"F1: {results['wav2vec']['f1']:.4f}\")\n\n# ============================================================\n# MODEL 2: HUBERT + CNN\n# ============================================================\nprint(f\"\\n{'*'*40}\")\nprint(\"MODEL 2: HUBERT + CNN\")\nprint(f\"{'*'*40}\")\n\ndataset_train = AudioEmotionDataset(\n    df_train_split, feature_extractor=hubert_extractor, mode='hubert'\n)\ndataset_eval = AudioEmotionDataset(\n    df_eval_split, feature_extractor=hubert_extractor, mode='hubert'\n)\n\nloader_train = DataLoader(\n    dataset_train,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    collate_fn=collate_hubert\n)\nloader_eval = DataLoader(\n    dataset_eval,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    collate_fn=collate_hubert\n)\n\nmodel2 = HubertCNNClassifier(num_labels=4).to(DEVICE)\nfreeze_backbone_except_last_n(model2, n=5, model_type='hubert')\n\noptimizer2 = torch.optim.AdamW(\n    model2.parameters(),\n    lr=CONFIG['learning_rate'],\n    weight_decay=CONFIG['weight_decay']\n)\n\ntrainer2 = Trainer(\n    model2,\n    loader_train,\n    loader_eval,\n    optimizer2,\n    DEVICE,\n    \"hubert_single_split\",\n    patience=CONFIG['patience']\n)\ntrainer2.train(CONFIG['epochs'])\n\nmodel2.eval()\nall_preds, all_labels = [], []\n\nwith torch.no_grad():\n    for batch in loader_eval:\n        batch = {k: v.to(DEVICE) for k, v in batch.items() if k != 'attention_mask'}\n        outputs = model2(**batch)\n        preds = torch.argmax(outputs['logits'], dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(batch['labels'].cpu().numpy())\n\nresults['hubert']['accuracy'] = accuracy_score(all_labels, all_preds)\nresults['hubert']['precision'] = precision_score(all_labels, all_preds, average='macro', zero_division=0)\nresults['hubert']['recall'] = recall_score(all_labels, all_preds, average='macro', zero_division=0)\nresults['hubert']['f1'] = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n\nprint(f\"HUBERT Acc: {results['hubert']['accuracy']:.4f} | \"\n      f\"F1: {results['hubert']['f1']:.4f}\")\n\n# ============================================================\n# MODEL 3: WHISPER + ATTENTION\n# ============================================================\nprint(f\"\\n{'*'*40}\")\nprint(\"MODEL 3: WHISPER + ATTENTION\")\nprint(f\"{'*'*40}\")\n\ndataset_train = AudioEmotionDataset(\n    df_train_split, feature_extractor=whisper_extractor, mode='whisper'\n)\ndataset_eval = AudioEmotionDataset(\n    df_eval_split, feature_extractor=whisper_extractor, mode='whisper'\n)\n\nloader_train = DataLoader(\n    dataset_train,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    collate_fn=collate_whisper\n)\nloader_eval = DataLoader(\n    dataset_eval,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    collate_fn=collate_whisper\n)\n\nmodel3 = WhisperAttentionClassifier(num_labels=4).to(DEVICE)\nfreeze_backbone_except_last_n(model3, n=3, model_type='whisper')\n\noptimizer3 = torch.optim.AdamW(\n    model3.parameters(),\n    lr=CONFIG['learning_rate'],\n    weight_decay=CONFIG['weight_decay']\n)\n\ntrainer3 = Trainer(\n    model3,\n    loader_train,\n    loader_eval,\n    optimizer3,\n    DEVICE,\n    \"whisper_single_split\",\n    patience=CONFIG['patience']\n)\ntrainer3.train(CONFIG['epochs'])\n\nmodel3.eval()\nall_preds, all_labels = [], []\n\nwith torch.no_grad():\n    for batch in loader_eval:\n        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n        outputs = model3(**batch)\n        preds = torch.argmax(outputs['logits'], dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(batch['labels'].cpu().numpy())\n\nresults['whisper']['accuracy'] = accuracy_score(all_labels, all_preds)\nresults['whisper']['precision'] = precision_score(all_labels, all_preds, average='macro', zero_division=0)\nresults['whisper']['recall'] = recall_score(all_labels, all_preds, average='macro', zero_division=0)\nresults['whisper']['f1'] = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n\nprint(f\"WHISPER Acc: {results['whisper']['accuracy']:.4f} | \"\n      f\"F1: {results['whisper']['f1']:.4f}\")\n\n# ============================================================\n# SUMMARY + SAVE\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"SUMMARY: SINGLE RANDOM HOLD-OUT RESULTS\")\nprint(\"=\"*60)\n\nfor model, metrics in results.items():\n    print(f\"\\n{model.upper()}:\")\n    for k, v in metrics.items():\n        print(f\"  {k:10s}: {v:.4f}\")\n\nwith open(f\"{OUTPUT_DIR}/single_random_holdout_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"\\nResults saved to: {OUTPUT_DIR}/single_random_holdout_results.json\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================\n# SUMMARY + SAVE\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"SUMMARY: SINGLE RANDOM HOLD-OUT RESULTS\")\nprint(\"=\"*60)\n\nfor model, metrics in results.items():\n    print(f\"\\n{model.upper()}:\")\n    for k, v in metrics.items():\n        print(f\"  {k:10s}: {v:.4f}\")\n\nwith open(f\"{OUTPUT_DIR}/single_random_holdout_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"\\nResults saved to: {OUTPUT_DIR}/single_random_holdout_results.json\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vis_data = []\n\nfor model_name in ['wav2vec', 'hubert', 'whisper']:\n    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n        vis_data.append({\n            'Model': model_name.upper(),\n            'Metric': metric,\n            'Value': results[model_name][metric]\n        })\n\ndf_vis = pd.DataFrame(vis_data)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=df_vis, x='Metric', y='Value', hue='Model')\nplt.ylim(0, 1)\nplt.title(\"Evaluation Metrics Comparison Across Models (Single Hold-out)\")\nplt.ylabel(\"Score\")\nplt.xlabel(\"Metric\")\nplt.legend(title=\"Model\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\ndef make_test_loader(df, processor, collate_fn, batch_size=16):\n    dataset = AudioEmotionDataset(df, processor=processor, mode=processor.mode)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\ntest_loaders = {\n    'Wav2Vec2': DataLoader(AudioEmotionDataset(df_test, processor=wav2vec_processor, mode='wav2vec'),\n                            batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_wav2vec),\n    'HubertCNN': DataLoader(AudioEmotionDataset(df_test, feature_extractor=hubert_extractor, mode='hubert'),\n                            batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_hubert),\n    'WhisperAttention': DataLoader(AudioEmotionDataset(df_test, feature_extractor=whisper_extractor, mode='whisper'),\n                                   batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_whisper)\n}\n\nmodels = {\n    'Wav2Vec2': model1,\n    'HubertCNN': model2,\n    'WhisperAttention': model3\n}\n\n# Đánh giá trên test\nfor model_name, model in models.items():\n    model.eval()\n    all_preds, all_labels = [], []\n\n    loader = test_loaders[model_name]\n    with torch.no_grad():\n        for batch in loader:\n            batch_device = {k: v.to(DEVICE) for k, v in batch.items() if k != 'attention_mask'}\n            outputs = model(**batch_device)\n            preds = torch.argmax(outputs['logits'], dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['labels'].cpu().numpy())\n\n    # Confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    cm_norm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n\n    plt.figure(figsize=(6,5))\n    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n                xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES)\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(f\"{model_name} - Test Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(f\"{OUTPUT_DIR}/{model_name}_test_confusion_matrix.png\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image, display\n\nfor model_name in ['wav2vec_single_split', 'hubert_single_split', 'whisper_single_split']:\n    print(f\"=== {model_name} ===\")\n    display(Image(filename=f\"{OUTPUT_DIR}/{model_name}_learning_curve.png\"))\n #   display(Image(filename=f\"{OUTPUT_DIR}/{model_name}_confusion_matrix.png\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CONFIG['batch_size'] = 2\nCONFIG['epochs'] = 6\n\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"PHASE 2: CONTINUED TRAINING ON DF_PHASE2 (IN-MEMORY)\")\nprint(\"=\"*60)\n\ndf_train_split = df_phase2.iloc[train_idx].reset_index(drop=True)\ndf_eval_split  = df_phase2.iloc[eval_idx].reset_index(drop=True)\n\nprint(f\"Phase2 Train: {len(df_train_split)} samples\")\nprint(f\"Phase2 Eval : {len(df_eval_split)} samples\")\n\nprint(\"\\n***** PHASE 2: WAV2VEC *****\")\n\ndataset_train = AudioEmotionDataset(\n    df_train_split, processor=wav2vec_processor, mode='wav2vec'\n)\ndataset_eval = AudioEmotionDataset(\n    df_eval_split, processor=wav2vec_processor, mode='wav2vec'\n)\n\ntrainer1.train_loader = DataLoader(\n    dataset_train,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    collate_fn=collate_wav2vec\n)\n\ntrainer1.val_loader = DataLoader(\n    dataset_eval,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    collate_fn=collate_wav2vec\n)\n\ntrainer1.model_name = \"wav2vec_phase2_continued\"\ntrainer1.train(CONFIG['epochs'] // 2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nCONFIG['epochs'] = 4\n\nCONFIG['batch_size'] = 2\n\nprint(\"\\n***** PHASE 2: HUBERT *****\")\n\ndataset_train = AudioEmotionDataset(\n    df_train_split, feature_extractor=hubert_extractor, mode='hubert'\n)\ndataset_eval = AudioEmotionDataset(\n    df_eval_split, feature_extractor=hubert_extractor, mode='hubert'\n)\n\ntrainer2.train_loader = DataLoader(\n    dataset_train,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    collate_fn=collate_hubert\n)\n\ntrainer2.val_loader = DataLoader(\n    dataset_eval,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    collate_fn=collate_hubert\n)\n\ntrainer2.model_name = \"hubert_phase2_continued\"\ntrainer2.train(CONFIG['epochs'] // 2)\n\n\nprint(\"\\n***** PHASE 2: WHISPER *****\")\n\ndataset_train = AudioEmotionDataset(\n    df_train_split, feature_extractor=whisper_extractor, mode='whisper'\n)\ndataset_eval = AudioEmotionDataset(\n    df_eval_split, feature_extractor=whisper_extractor, mode='whisper'\n)\n\ntrainer3.train_loader = DataLoader(\n    dataset_train,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    collate_fn=collate_whisper\n)\n\ntrainer3.val_loader = DataLoader(\n    dataset_eval,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    collate_fn=collate_whisper\n)\n\ntrainer3.model_name = \"whisper_phase2_continued\"\ntrainer3.train(CONFIG['epochs'] // 2)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\ndef make_test_loader(df, processor, collate_fn, batch_size=16):\n    dataset = AudioEmotionDataset(df, processor=processor, mode=processor.mode)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\ntest_loaders = {\n    'Wav2Vec2': DataLoader(AudioEmotionDataset(df_test, processor=wav2vec_processor, mode='wav2vec'),\n                            batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_wav2vec),\n    'HubertCNN': DataLoader(AudioEmotionDataset(df_test, feature_extractor=hubert_extractor, mode='hubert'),\n                            batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_hubert),\n    'WhisperAttention': DataLoader(AudioEmotionDataset(df_test, feature_extractor=whisper_extractor, mode='whisper'),\n                                   batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_whisper)\n}\n\nmodels = {\n    'Wav2Vec2': model1,\n    'HubertCNN': model2,\n    'WhisperAttention': model3\n}\n\n# Đánh giá trên test\nfor model_name, model in models.items():\n    model.eval()\n    all_preds, all_labels = [], []\n\n    loader = test_loaders[model_name]\n    with torch.no_grad():\n        for batch in loader:\n            batch_device = {k: v.to(DEVICE) for k, v in batch.items() if k != 'attention_mask'}\n            outputs = model(**batch_device)\n            preds = torch.argmax(outputs['logits'], dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['labels'].cpu().numpy())\n\n    # Confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    cm_norm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n\n    plt.figure(figsize=(6,5))\n    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n                xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES)\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(f\"{model_name} - Test Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(f\"{OUTPUT_DIR}/{model_name}_phase2_test_confusion_matrix.png\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch.utils.data import DataLoader\n\n# --- Hàm tạo DataLoader cho test ---\ndef make_test_loader(df, processor, collate_fn, batch_size=16):\n    dataset = AudioEmotionDataset(df, processor=processor, mode=processor.mode)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n# --- Test loaders cho 3 mô hình ---\ntest_loaders = {\n    'Wav2Vec2': DataLoader(\n        AudioEmotionDataset(df_test, processor=wav2vec_processor, mode='wav2vec'),\n        batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_wav2vec\n    ),\n    'HubertCNN': DataLoader(\n        AudioEmotionDataset(df_test, feature_extractor=hubert_extractor, mode='hubert'),\n        batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_hubert\n    ),\n    'WhisperAttention': DataLoader(\n        AudioEmotionDataset(df_test, feature_extractor=whisper_extractor, mode='whisper'),\n        batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_whisper\n    )\n}\n\n# --- Các model ---\nmodels = {\n    'Wav2Vec2': model1,\n    'HubertCNN': model2,\n    'WhisperAttention': model3\n}\n\n# --- In summary header ---\nprint(\"SUMMARY: SINGLE RANDOM HOLD-OUT RESULTS\")\nprint(\"=\"*60)\n\n# --- Đánh giá ---\nfor model_name, model in models.items():\n    model.eval()\n    all_preds, all_labels = [], []\n\n    loader = test_loaders[model_name]\n    with torch.no_grad():\n        for batch in loader:\n            # Chuyển sang device\n            batch_device = {k: v.to(DEVICE) for k, v in batch.items() if k != 'attention_mask'}\n            outputs = model(**batch_device)\n            preds = torch.argmax(outputs['logits'], dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['labels'].cpu().numpy())\n\n    # Tính metric\n    acc = accuracy_score(all_labels, all_preds)\n    prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n    rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n\n    # In kết quả\n    print(f\"{model_name}:\")\n    print(f\"  accuracy  : {acc:.4f}\")\n    print(f\"  precision : {prec:.4f}\")\n    print(f\"  recall    : {rec:.4f}\")\n    print(f\"  f1        : {f1:.4f}\")\n    print()\n\n    # Confusion matrix normalized\n    cm = confusion_matrix(all_labels, all_preds)\n    cm_norm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n\n    plt.figure(figsize=(6,5))\n    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n                xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES)\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(f\"{model_name} - Test Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(f\"{OUTPUT_DIR}/{model_name}_phase2_test_confusion_matrix.png\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image, display\n\nfor model_name in ['wav2vec_phase2_continued', 'hubert_phase2_continued', 'whisper_phase2_continued']:\n    print(f\"=== {model_name} ===\")\n    display(Image(filename=f\"{OUTPUT_DIR}/{model_name}_learning_curve.png\"))\n    # display(Image(filename=f\"{OUTPUT_DIR}/{model_name}_confusion_matrix.png\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Đổi tên hiển thị và tên file\ntrainer1.model_name = \"Wav2Vec2\"  # bỏ _phase2_continued\n\n# Vẽ lại learning curve từ history cũ\ntrainer1.plot_learning_curves()\n\n# Hiển thị\nfrom IPython.display import Image, display\ndisplay(Image(filename=f\"{OUTPUT_DIR}/{trainer1.model_name}_learning_curve.png\"))\n\n# Đổi tên hiển thị và tên file\ntrainer2.model_name = \"HubertCNN\"  # bỏ _phase2_continued\n\n# Vẽ lại learning curve từ history cũ\ntrainer2.plot_learning_curves()\n\n# Hiển thị\nfrom IPython.display import Image, display\ndisplay(Image(filename=f\"{OUTPUT_DIR}/{trainer2.model_name}_learning_curve.png\"))\n\n\n\n# Đổi tên hiển thị và tên file\ntrainer3.model_name = \"WhisperAttention\"  # bỏ _phase2_continued\n\n# Vẽ lại learning curve từ history cũ\ntrainer3.plot_learning_curves()\n\n# Hiển thị\nfrom IPython.display import Image, display\ndisplay(Image(filename=f\"{OUTPUT_DIR}/{trainer3.model_name}_learning_curve.png\"))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Giả sử bạn vẫn còn trainer với history cũ\ntrainer.model_name = \"model1\"  # tên mới hiển thị trên biểu đồ\n\nepochs = range(1, len(trainer.history['train_loss']) + 1)\n\nplt.figure(figsize=(12, 4))\n\n# Loss\nplt.subplot(1, 3, 1)\nplt.plot(epochs, trainer.history['train_loss'], label='Train Loss')\nplt.plot(epochs, trainer.history['val_loss'], label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title(f'{trainer.model_name} - Loss')  # tên mới\nplt.legend()\n\n# Accuracy\nplt.subplot(1, 3, 2)\nplt.plot(epochs, trainer.history['train_acc'], label='Train Acc')\nplt.plot(epochs, trainer.history['val_acc'], label='Val Acc')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title(f'{trainer.model_name} - Accuracy')\nplt.legend()\n\n# F1\nplt.subplot(1, 3, 3)\nplt.plot(epochs, trainer.history['train_f1'], label='Train F1')\nplt.plot(epochs, trainer.history['val_f1'], label='Val F1')\nplt.xlabel('Epoch')\nplt.ylabel('F1-score')\nplt.title(f'{trainer.model_name} - F1')\nplt.legend()\n\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_DIR}/{trainer.model_name}_learning_curve.png\")  # lưu file mới\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# --- Đánh giá từng model ---\nfor model_name, model in models.items():\n    model.eval()\n    all_preds, all_labels = [], []\n\n    loader = test_loaders[model_name]\n    with torch.no_grad():\n        for batch in loader:\n            batch_device = {k: v.to(DEVICE) for k, v in batch.items() if k != 'attention_mask'}\n            outputs = model(**batch_device)\n            preds = torch.argmax(outputs['logits'], dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['labels'].cpu().numpy())\n\n    # --- Metric tổng quát ---\n    acc = accuracy_score(all_labels, all_preds)\n    prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n    rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n\n    print(\"=\"*40)\n    print(f\"{model_name} - SUMMARY: SINGLE RANDOM HOLD-OUT RESULTS\")\n    print(\"=\"*40)\n    print(f\"accuracy  : {acc:.4f}\")\n    print(f\"precision : {prec:.4f}\")\n    print(f\"recall    : {rec:.4f}\")\n    print(f\"f1        : {f1:.4f}\")\n    print()\n\n    # --- Classification report chi tiết từng lớp ---\n    print(\"=\"*40)\n    print(f\"{model_name} - BÁO CÁO CHI TIẾT TỪNG LỚP\")\n    print(\"=\"*40)\n    print(classification_report(all_labels, all_preds, target_names=EMOTION_CLASSES, digits=2))\n    print()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\n\n# Tạo thư mục final nếu chưa có\nFINAL_DIR = \"final\"\nos.makedirs(FINAL_DIR, exist_ok=True)\n\n# Lưu mô hình\ntorch.save(model1.state_dict(), os.path.join(FINAL_DIR, \"wav2vec_phase2_final.pt\"))\ntorch.save(model2.state_dict(), os.path.join(FINAL_DIR, \"hubert_phase2_final.pt\"))\ntorch.save(model3.state_dict(), os.path.join(FINAL_DIR, \"whisper_phase2_final.pt\"))\n\nprint(f\"Models saved to {FINAL_DIR}/\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}