{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KHAI BÁO THƯ VIỆN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, Wav2Vec2Model,\n",
    "    AutoFeatureExtractor, HubertModel,\n",
    "    WhisperFeatureExtractor, WhisperModel\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, \n",
    "    recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torchaudio\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# DEVICE & CONSTANTS\n",
    "# ============================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"DEVICE = {DEVICE}\")\n",
    "\n",
    "DATA_DIR = \"/kaggle/input/vietnamese-speech-emotion-recognition-dataset\"\n",
    "OUTPUT_DIR = \"models\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "EMOTION_CLASSES = ['happy', 'neutral', 'sad', 'angry']\n",
    "EMOTION_MAP = {label: idx for idx, label in enumerate(EMOTION_CLASSES)}\n",
    "\n",
    "# Config for training\n",
    "CONFIG = {\n",
    "    'min_duration': 0.5,\n",
    "    'max_duration': 15.0,\n",
    "    'sample_rate': 16000,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 5,\n",
    "    'warmup_steps': 500,\n",
    "    'learning_rate': 1e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'patience': 5,\n",
    "    'random_seed': 42,\n",
    "}\n",
    "\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "np.random.seed(CONFIG['random_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: LOAD DATA FROM FOLDER STRUCTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def load_audio_dataset(data_dir):\n",
    "    datasets = {}\n",
    "    \n",
    "    for split in ['train/phase1', 'train/phase2', 'val', 'test']:\n",
    "        split_path = os.path.join(data_dir, split)\n",
    "        data = []\n",
    "        \n",
    "        if not os.path.exists(split_path):\n",
    "            print(f\"Warning: {split_path} not found!\")\n",
    "            continue\n",
    "        \n",
    "        for emotion in EMOTION_CLASSES:\n",
    "            emotion_dir = os.path.join(split_path, emotion)\n",
    "            \n",
    "            if not os.path.exists(emotion_dir):\n",
    "                continue\n",
    "            \n",
    "            for filename in os.listdir(emotion_dir):\n",
    "                if filename.endswith('.wav'):\n",
    "                    filepath = os.path.join(emotion_dir, filename)\n",
    "                    try:\n",
    "                        # Load audio & calculate duration\n",
    "                        waveform, sr = torchaudio.load(filepath)\n",
    "                        \n",
    "                        # Resample if needed\n",
    "                        if sr != CONFIG['sample_rate']:\n",
    "                            waveform = torchaudio.functional.resample(\n",
    "                                waveform, sr, CONFIG['sample_rate']\n",
    "                            )\n",
    "                            sr = CONFIG['sample_rate']\n",
    "                        \n",
    "                        duration = waveform.shape[1] / sr\n",
    "                        \n",
    "                        # Filter by duration\n",
    "                        if CONFIG['min_duration'] <= duration <= CONFIG['max_duration']:\n",
    "                            data.append({\n",
    "                                'audio_path': filepath,\n",
    "                                'emotion': emotion,\n",
    "                                'emotion_id': EMOTION_MAP[emotion],\n",
    "                                'duration': duration,\n",
    "                                'split': split.split('/') if '/' in split else split\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {filepath}: {e}\")\n",
    "        \n",
    "        if data:\n",
    "            datasets[split] = pd.DataFrame(data)\n",
    "            print(f\"{split}: {len(data)} files\")\n",
    "            print(f\"  Emotion distribution: {datasets[split]['emotion'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "datasets = load_audio_dataset(DATA_DIR)\n",
    "\n",
    "# Check if we have data\n",
    "df_phase1 = datasets.get('train/phase1', pd.DataFrame())\n",
    "df_phase2 = datasets.get('train/phase2', pd.DataFrame())\n",
    "df_val = datasets.get('val', pd.DataFrame())\n",
    "df_test = datasets.get('test', pd.DataFrame())\n",
    "\n",
    "print(f\"\\nTotal samples:\")\n",
    "print(f\"  Phase1: {len(df_phase1)}\")\n",
    "print(f\"  Phase2: {len(df_phase2)}\")\n",
    "print(f\"  Val: {len(df_val)}\")\n",
    "print(f\"  Test: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: DATASET CLASS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class AudioEmotionDataset(Dataset):\n",
    "    \"\"\"Load audio files and compute features\"\"\"\n",
    "    def __init__(self, df, processor=None, feature_extractor=None, mode='wav2vec'):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_path = row['audio_path']\n",
    "        label = row['emotion_id']\n",
    "        \n",
    "        # Load audio\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Ensure mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(0, keepdim=True)\n",
    "        \n",
    "        waveform = waveform.squeeze(0).numpy().astype(np.float32)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != CONFIG['sample_rate']:\n",
    "            waveform = torchaudio.functional.resample(\n",
    "                torch.from_numpy(waveform), sr, CONFIG['sample_rate']\n",
    "            ).numpy()\n",
    "        \n",
    "        return {\n",
    "            'waveform': waveform,\n",
    "            'labels': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLATE (Wav2Vec2, HuBERT, Whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nSetting up collate functions...\")\n",
    "\n",
    "# WAV2VEC2 COLLATE\n",
    "wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "def collate_wav2vec(batch):\n",
    "    waveforms = [item['waveform'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Process with padding\n",
    "    padded = wav2vec_processor(\n",
    "        waveforms,\n",
    "        sampling_rate=CONFIG['sample_rate'],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_values': padded['input_values'],\n",
    "        'labels': torch.tensor(labels, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# HUBERT COLLATE\n",
    "hubert_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "\n",
    "def collate_hubert(batch):\n",
    "    waveforms = [item['waveform'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Extract features with padding\n",
    "    features = hubert_extractor(\n",
    "        waveforms,\n",
    "        sampling_rate=CONFIG['sample_rate'],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_values': features['input_values'],\n",
    "        'labels': torch.tensor(labels, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# WHISPER COLLATE\n",
    "whisper_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "def collate_whisper(batch):\n",
    "    waveforms = [item['waveform'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Extract mel spectrogram\n",
    "    features = whisper_extractor(\n",
    "        waveforms,\n",
    "        sampling_rate=CONFIG['sample_rate'],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Pad to max length\n",
    "    input_features = features['input_features']\n",
    "    max_len = 3000\n",
    "    \n",
    "    padded = []\n",
    "    for feat in input_features:\n",
    "        if feat.shape[-1] < max_len:\n",
    "            pad = torch.nn.functional.pad(\n",
    "                feat, (0, max_len - feat.shape[-1]), mode='constant', value=0\n",
    "            )\n",
    "        else:\n",
    "            pad = feat[:, :max_len]\n",
    "        padded.append(pad)\n",
    "    \n",
    "    return {\n",
    "        'input_features': torch.stack(padded),\n",
    "        'labels': torch.tensor(labels, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KIẾN TRÚC MÔ HÌNH (Wav2Vec2, HuBERT CNN, Whisper Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nLoading model architectures...\")\n",
    "\n",
    "# --- MODEL 1: Wav2Vec2 + MLP ---\n",
    "class Wav2VecClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=4):\n",
    "        super().__init__()\n",
    "        self.encoder = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        hidden_size = self.encoder.config.hidden_size  # 768\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_values, attention_mask=None, labels=None):\n",
    "        # Encode\n",
    "        out = self.encoder(input_values=input_values, attention_mask=attention_mask)\n",
    "        hidden = out.last_hidden_state  # [B, T, 768]\n",
    "        \n",
    "        # Mean pooling\n",
    "        if attention_mask is not None:\n",
    "            # Mask padding tokens\n",
    "            mask = attention_mask.unsqueeze(-1).float()  # [B, T, 1]\n",
    "            pooled = (hidden * mask).sum(1) / mask.sum(1)\n",
    "        else:\n",
    "            pooled = hidden.mean(dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "        return {'logits': logits, 'loss': loss}\n",
    "\n",
    "# --- MODEL 2: HuBERT + CNN ---\n",
    "class HubertCNNClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=4):\n",
    "        super().__init__()\n",
    "        self.encoder = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "        hidden_size = self.encoder.config.hidden_size  # 768\n",
    "        \n",
    "        # CNN 1D head\n",
    "        self.conv1 = nn.Conv1d(hidden_size, 256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.conv2 = nn.Conv1d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_values, attention_mask=None, labels=None):\n",
    "        # Encode\n",
    "        out = self.encoder(input_values=input_values, attention_mask=attention_mask)\n",
    "        hidden = out.last_hidden_state  # [B, T, 768]\n",
    "        \n",
    "        # CNN: transpose [B, T, 768] -> [B, 768, T]\n",
    "        x = hidden.transpose(1, 2)\n",
    "        \n",
    "        # Conv blocks\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Global max pooling\n",
    "        x = torch.max(x, dim=2).values  # [B, 128]\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "        return {'logits': logits, 'loss': loss}\n",
    "\n",
    "# --- MODEL 3: Whisper + Attention ---\n",
    "class WhisperAttentionClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=4):\n",
    "        super().__init__()\n",
    "        self.encoder = WhisperModel.from_pretrained(\"openai/whisper-tiny\").encoder\n",
    "        hidden_size = 384  # Whisper-tiny\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attn_query = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_features, labels=None):\n",
    "        # Encode\n",
    "        out = self.encoder(input_features=input_features)\n",
    "        hidden = out.last_hidden_state  # [B, T, 384]\n",
    "        \n",
    "        # Attention\n",
    "        attn_scores = self.attn_query(hidden)  # [B, T, 1]\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)  # [B, T, 1]\n",
    "        \n",
    "        # Context vector\n",
    "        context = (attn_weights * hidden).sum(dim=1)  # [B, 384]\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.fc(context)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "        return {'logits': logits, 'loss': loss}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFUSION MATRIX VÀ LEARNING CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(self, labels, preds, normalize=True):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\".2f\" if normalize else \"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=EMOTION_CLASSES,\n",
    "        yticklabels=EMOTION_CLASSES\n",
    "    )\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"{self.model_name} - Confusion Matrix\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/{self.model_name}_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_learning_curves(self):\n",
    "    epochs = range(1, len(self.history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 4))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, self.history['train_loss'], marker='o', label='Train')\n",
    "    plt.plot(epochs, self.history['val_loss'], marker='o', label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, self.history['train_acc'], marker='o', label='Train')\n",
    "    plt.plot(epochs, self.history['val_acc'], marker='o', label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # F1\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, self.history['train_f1'], marker='o', label='Train')\n",
    "    plt.plot(epochs, self.history['val_f1'], marker='o', label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1-score')\n",
    "    plt.title('F1-score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/{self.model_name}_learning_curve.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, device, model_name, patience=5):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.patience = patience\n",
    "        self.history = {\n",
    "            'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "            'val_loss': [], 'val_acc': [], 'val_f1': []\n",
    "        }\n",
    "        self.best_val_acc = 0\n",
    "        self.patience_counter = 0\n",
    "        self.last_val_preds = None\n",
    "        self.last_val_labels = None\n",
    "\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\", leave=False):\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items() if k != 'attention_mask'}\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(**batch)\n",
    "            loss = outputs['loss']\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(outputs['logits'], dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        return train_loss / len(self.train_loader), train_acc, train_f1\n",
    "    \n",
    "    def eval_epoch(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Evaluating\", leave=False):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items() if k != 'attention_mask'}\n",
    "                \n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs['loss']\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs['logits'], dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        return val_loss / len(self.val_loader), val_acc, val_f1, all_preds, all_labels\n",
    "    \n",
    "    def plot_learning_curves(self):\n",
    "        epochs = range(1, len(self.history['train_loss']) + 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # --------- Loss ----------\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(epochs, self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(epochs, self.history['val_loss'], label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'{self.model_name} - Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # --------- Accuracy ----------\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(epochs, self.history['train_acc'], label='Train Acc')\n",
    "        plt.plot(epochs, self.history['val_acc'], label='Val Acc')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'{self.model_name} - Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # --------- F1 ----------\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(epochs, self.history['train_f1'], label='Train F1')\n",
    "        plt.plot(epochs, self.history['val_f1'], label='Val F1')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1-score')\n",
    "        plt.title(f'{self.model_name} - F1')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{OUTPUT_DIR}/{self.model_name}_learning_curve.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc, train_f1 = self.train_epoch()\n",
    "            val_loss, val_acc, val_f1, _, _ = self.eval_epoch()\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['train_f1'].append(train_f1)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} F1: {train_f1:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} F1: {val_f1:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                self.patience_counter = 0\n",
    "                torch.save(self.model.state_dict(), f\"{OUTPUT_DIR}/{self.model_name}_best.pt\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                if self.patience_counter >= self.patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # vẽ learning curve sau khi train xong\n",
    "        self.plot_learning_curves()\n",
    "        \n",
    "        return self.history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FREEZE LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Freezing backbone và unfreeze 3 layers cuối ---\n",
    "\n",
    "def freeze_backbone_except_last_n(model, n=3, model_type='wav2vec'):\n",
    "    \"\"\"\n",
    "    model: nn.Module (Wav2Vec2Classifier / HubertCNNClassifier / WhisperAttentionClassifier)\n",
    "    n: số layer cuối để unfreeze\n",
    "    model_type: 'wav2vec', 'hubert', 'whisper'\n",
    "    \"\"\"\n",
    "    #todo demo\n",
    "    if model_type == 'wav2vec':\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for layer in model.encoder.encoder.layers[-n:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    elif model_type == 'hubert':\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for layer in model.encoder.encoder.layers[-n:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    elif model_type == 'whisper':\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # mở n layer cuối encoder\n",
    "        for layer in model.encoder.layers[-n:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHASE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: SINGLE RANDOM SHUFFLE SPLIT ON PHASE1 (NO STRATIFY)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# SINGLE RANDOM SPLIT (ONE TIME ONLY)\n",
    "# ============================================================\n",
    "test_size = 0.2  # 20% for evaluation\n",
    "n_samples = len(df_phase1)\n",
    "n_eval = int(n_samples * test_size)\n",
    "\n",
    "rng = np.random.RandomState(CONFIG['random_seed'])\n",
    "indices = rng.permutation(n_samples)\n",
    "\n",
    "eval_idx = indices[:n_eval]\n",
    "train_idx = indices[n_eval:]\n",
    "\n",
    "df_train_split = df_phase1.iloc[train_idx].reset_index(drop=True)\n",
    "df_eval_split = df_phase1.iloc[eval_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(df_train_split)} samples\")\n",
    "print(f\"Eval: {len(df_eval_split)} samples\")\n",
    "\n",
    "# ============================================================\n",
    "# INIT RESULT CONTAINER (NO LISTS)\n",
    "# ============================================================\n",
    "results = {\n",
    "    'wav2vec': {},\n",
    "    'hubert': {},\n",
    "    'whisper': {}\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# MODEL 1: WAV2VEC 2.0\n",
    "# ============================================================\n",
    "print(f\"\\n{'*'*40}\")\n",
    "print(\"MODEL 1: WAV2VEC 2.0\")\n",
    "print(f\"{'*'*40}\")\n",
    "\n",
    "dataset_train = AudioEmotionDataset(\n",
    "    df_train_split, processor=wav2vec_processor, mode='wav2vec'\n",
    ")\n",
    "dataset_eval = AudioEmotionDataset(\n",
    "    df_eval_split, processor=wav2vec_processor, mode='wav2vec'\n",
    ")\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_wav2vec\n",
    ")\n",
    "loader_eval = DataLoader(\n",
    "    dataset_eval,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_wav2vec\n",
    ")\n",
    "\n",
    "model1 = Wav2VecClassifier(num_labels=4).to(DEVICE)\n",
    "freeze_backbone_except_last_n(model1, n=1, model_type='wav2vec')\n",
    "\n",
    "optimizer1 = torch.optim.AdamW(\n",
    "    model1.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model1,\n",
    "    loader_train,\n",
    "    loader_eval,\n",
    "    optimizer1,\n",
    "    DEVICE,\n",
    "    \"wav2vec_single_split\",\n",
    "    patience=CONFIG['patience']\n",
    ")\n",
    "trainer1.train(CONFIG['epochs'])\n",
    "\n",
    "model1.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader_eval:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items() if k != 'attention_mask'}\n",
    "        outputs = model1(**batch)\n",
    "        preds = torch.argmax(outputs['logits'], dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "results['wav2vec']['accuracy'] = accuracy_score(all_labels, all_preds)\n",
    "results['wav2vec']['precision'] = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "results['wav2vec']['recall'] = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "results['wav2vec']['f1'] = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"WAV2VEC Acc: {results['wav2vec']['accuracy']:.4f} | \"\n",
    "      f\"F1: {results['wav2vec']['f1']:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL 2: HUBERT + CNN\n",
    "# ============================================================\n",
    "print(f\"\\n{'*'*40}\")\n",
    "print(\"MODEL 2: HUBERT + CNN\")\n",
    "print(f\"{'*'*40}\")\n",
    "\n",
    "dataset_train = AudioEmotionDataset(\n",
    "    df_train_split, feature_extractor=hubert_extractor, mode='hubert'\n",
    ")\n",
    "dataset_eval = AudioEmotionDataset(\n",
    "    df_eval_split, feature_extractor=hubert_extractor, mode='hubert'\n",
    ")\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_hubert\n",
    ")\n",
    "loader_eval = DataLoader(\n",
    "    dataset_eval,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_hubert\n",
    ")\n",
    "\n",
    "model2 = HubertCNNClassifier(num_labels=4).to(DEVICE)\n",
    "freeze_backbone_except_last_n(model2, n=5, model_type='hubert')\n",
    "\n",
    "optimizer2 = torch.optim.AdamW(\n",
    "    model2.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model2,\n",
    "    loader_train,\n",
    "    loader_eval,\n",
    "    optimizer2,\n",
    "    DEVICE,\n",
    "    \"hubert_single_split\",\n",
    "    patience=CONFIG['patience']\n",
    ")\n",
    "trainer2.train(CONFIG['epochs'])\n",
    "\n",
    "model2.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader_eval:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items() if k != 'attention_mask'}\n",
    "        outputs = model2(**batch)\n",
    "        preds = torch.argmax(outputs['logits'], dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "results['hubert']['accuracy'] = accuracy_score(all_labels, all_preds)\n",
    "results['hubert']['precision'] = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "results['hubert']['recall'] = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "results['hubert']['f1'] = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"HUBERT Acc: {results['hubert']['accuracy']:.4f} | \"\n",
    "      f\"F1: {results['hubert']['f1']:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL 3: WHISPER + ATTENTION\n",
    "# ============================================================\n",
    "print(f\"\\n{'*'*40}\")\n",
    "print(\"MODEL 3: WHISPER + ATTENTION\")\n",
    "print(f\"{'*'*40}\")\n",
    "\n",
    "dataset_train = AudioEmotionDataset(\n",
    "    df_train_split, feature_extractor=whisper_extractor, mode='whisper'\n",
    ")\n",
    "dataset_eval = AudioEmotionDataset(\n",
    "    df_eval_split, feature_extractor=whisper_extractor, mode='whisper'\n",
    ")\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_whisper\n",
    ")\n",
    "loader_eval = DataLoader(\n",
    "    dataset_eval,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_whisper\n",
    ")\n",
    "\n",
    "model3 = WhisperAttentionClassifier(num_labels=4).to(DEVICE)\n",
    "freeze_backbone_except_last_n(model3, n=3, model_type='whisper')\n",
    "\n",
    "optimizer3 = torch.optim.AdamW(\n",
    "    model3.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "trainer3 = Trainer(\n",
    "    model3,\n",
    "    loader_train,\n",
    "    loader_eval,\n",
    "    optimizer3,\n",
    "    DEVICE,\n",
    "    \"whisper_single_split\",\n",
    "    patience=CONFIG['patience']\n",
    ")\n",
    "trainer3.train(CONFIG['epochs'])\n",
    "\n",
    "model3.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader_eval:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        outputs = model3(**batch)\n",
    "        preds = torch.argmax(outputs['logits'], dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "results['whisper']['accuracy'] = accuracy_score(all_labels, all_preds)\n",
    "results['whisper']['precision'] = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "results['whisper']['recall'] = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "results['whisper']['f1'] = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"WHISPER Acc: {results['whisper']['accuracy']:.4f} | \"\n",
    "      f\"F1: {results['whisper']['f1']:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY + SAVE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: SINGLE RANDOM HOLD-OUT RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model.upper()}:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k:10s}: {v:.4f}\")\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/single_random_holdout_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {OUTPUT_DIR}/single_random_holdout_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# SUMMARY + SAVE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: SINGLE RANDOM HOLD-OUT RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model.upper()}:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k:10s}: {v:.4f}\")\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/single_random_holdout_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {OUTPUT_DIR}/single_random_holdout_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vis_data = []\n",
    "\n",
    "for model_name in ['wav2vec', 'hubert', 'whisper']:\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "        vis_data.append({\n",
    "            'Model': model_name.upper(),\n",
    "            'Metric': metric,\n",
    "            'Value': results[model_name][metric]\n",
    "        })\n",
    "\n",
    "df_vis = pd.DataFrame(vis_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_vis, x='Metric', y='Value', hue='Model')\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Evaluation Metrics Comparison Across Models (Single Hold-out)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_test_loader(df, processor, collate_fn, batch_size=16):\n",
    "    dataset = AudioEmotionDataset(df, processor=processor, mode=processor.mode)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_loaders = {\n",
    "    'Wav2Vec2': DataLoader(AudioEmotionDataset(df_test, processor=wav2vec_processor, mode='wav2vec'),\n",
    "                            batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_wav2vec),\n",
    "    'HubertCNN': DataLoader(AudioEmotionDataset(df_test, feature_extractor=hubert_extractor, mode='hubert'),\n",
    "                            batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_hubert),\n",
    "    'WhisperAttention': DataLoader(AudioEmotionDataset(df_test, feature_extractor=whisper_extractor, mode='whisper'),\n",
    "                                   batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_whisper)\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Wav2Vec2': model1,\n",
    "    'HubertCNN': model2,\n",
    "    'WhisperAttention': model3\n",
    "}\n",
    "\n",
    "# Đánh giá trên test\n",
    "for model_name, model in models.items():\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    loader = test_loaders[model_name]\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch_device = {k: v.to(DEVICE) for k, v in batch.items() if k != 'attention_mask'}\n",
    "            outputs = model(**batch_device)\n",
    "            preds = torch.argmax(outputs['logits'], dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"{model_name} - Test Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/{model_name}_test_confusion_matrix.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for model_name in ['wav2vec_single_split', 'hubert_single_split', 'whisper_single_split']:\n",
    "    print(f\"=== {model_name} ===\")\n",
    "    display(Image(filename=f\"{OUTPUT_DIR}/{model_name}_learning_curve.png\"))\n",
    " #   display(Image(filename=f\"{OUTPUT_DIR}/{model_name}_confusion_matrix.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHASE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: CONTINUED TRAINING ON DF_PHASE2 (IN-MEMORY)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_train_split = df_phase2.iloc[train_idx].reset_index(drop=True)\n",
    "df_eval_split  = df_phase2.iloc[eval_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"Phase2 Train: {len(df_train_split)} samples\")\n",
    "print(f\"Phase2 Eval : {len(df_eval_split)} samples\")\n",
    "\n",
    "print(\"\\n***** PHASE 2: WAV2VEC *****\")\n",
    "\n",
    "dataset_train = AudioEmotionDataset(\n",
    "    df_train_split, processor=wav2vec_processor, mode='wav2vec'\n",
    ")\n",
    "dataset_eval = AudioEmotionDataset(\n",
    "    df_eval_split, processor=wav2vec_processor, mode='wav2vec'\n",
    ")\n",
    "\n",
    "trainer1.train_loader = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_wav2vec\n",
    ")\n",
    "\n",
    "trainer1.val_loader = DataLoader(\n",
    "    dataset_eval,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_wav2vec\n",
    ")\n",
    "\n",
    "trainer1.model_name = \"wav2vec_phase2_continued\"\n",
    "trainer1.train(CONFIG['epochs'] // 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n***** PHASE 2: HUBERT *****\")\n",
    "\n",
    "dataset_train = AudioEmotionDataset(\n",
    "    df_train_split, feature_extractor=hubert_extractor, mode='hubert'\n",
    ")\n",
    "dataset_eval = AudioEmotionDataset(\n",
    "    df_eval_split, feature_extractor=hubert_extractor, mode='hubert'\n",
    ")\n",
    "\n",
    "trainer2.train_loader = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_hubert\n",
    ")\n",
    "\n",
    "trainer2.val_loader = DataLoader(\n",
    "    dataset_eval,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_hubert\n",
    ")\n",
    "\n",
    "trainer2.model_name = \"hubert_phase2_continued\"\n",
    "trainer2.train(CONFIG['epochs'] // 2)\n",
    "\n",
    "\n",
    "print(\"\\n***** PHASE 2: WHISPER *****\")\n",
    "\n",
    "dataset_train = AudioEmotionDataset(\n",
    "    df_train_split, feature_extractor=whisper_extractor, mode='whisper'\n",
    ")\n",
    "dataset_eval = AudioEmotionDataset(\n",
    "    df_eval_split, feature_extractor=whisper_extractor, mode='whisper'\n",
    ")\n",
    "\n",
    "trainer3.train_loader = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_whisper\n",
    ")\n",
    "\n",
    "trainer3.val_loader = DataLoader(\n",
    "    dataset_eval,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_whisper\n",
    ")\n",
    "\n",
    "trainer3.model_name = \"whisper_phase2_continued\"\n",
    "trainer3.train(CONFIG['epochs'] // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_test_loader(df, processor, collate_fn, batch_size=16):\n",
    "    dataset = AudioEmotionDataset(df, processor=processor, mode=processor.mode)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_loaders = {\n",
    "    'Wav2Vec2': DataLoader(AudioEmotionDataset(df_test, processor=wav2vec_processor, mode='wav2vec'),\n",
    "                            batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_wav2vec),\n",
    "    'HubertCNN': DataLoader(AudioEmotionDataset(df_test, feature_extractor=hubert_extractor, mode='hubert'),\n",
    "                            batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_hubert),\n",
    "    'WhisperAttention': DataLoader(AudioEmotionDataset(df_test, feature_extractor=whisper_extractor, mode='whisper'),\n",
    "                                   batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_whisper)\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Wav2Vec2': model1,\n",
    "    'HubertCNN': model2,\n",
    "    'WhisperAttention': model3\n",
    "}\n",
    "\n",
    "# Đánh giá trên test\n",
    "for model_name, model in models.items():\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    loader = test_loaders[model_name]\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch_device = {k: v.to(DEVICE) for k, v in batch.items() if k != 'attention_mask'}\n",
    "            outputs = model(**batch_device)\n",
    "            preds = torch.argmax(outputs['logits'], dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"{model_name} - Test Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/{model_name}_phase2_test_confusion_matrix.png\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9021209,
     "sourceId": 14153892,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
